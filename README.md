# nlpforfrench
nlp ressources for french

http://nlp.polytechnique.fr/

BARthez, the first french sequence to sequence pretrained model pretrained on 66GB of french raw text for roughly 60 hours on 128 Nvidia V100 GPUs.
French Word2vec vectors of dimension 300 that were trained using CBOW on a huge 33GB French raw text that we crawled and pre-processed from the French web.
BERTweetFR, the first pre-trained large scale language model adapted to French tweets. It is initialized with CamemBERT, the state-of-art general-domain language model for French based on the RoBERTa architecture. We perform domain-adaptive pre-training on 182M deduplicated tweets. The training runs for roughly 20 hours on 8 Nvidia V100 GPUs
JuriBERT, set of different size BERT models pre-trained from scratch on 6.3GB of French legal-domain corpora.
